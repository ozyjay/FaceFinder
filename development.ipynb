{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Real-time Emotion and Age Detection from Webcam Feed\n",
    "\n",
    "This notebook demonstrates how to use pre-trained models from Hugging Face and FastAI for real-time emotion and age detection using a webcam feed.\n",
    "\n",
    "## Importing Libraries\n",
    "\n",
    "First, we need to import the necessary libraries. We are using OpenCV for capturing video frames from the webcam, PyTorch for running inference on the pre-trained models, and Hugging Face Transformers for loading the pre-trained models.\n",
    "\n",
    "We use the logging module to display information and error messages with timestamps.\n"
   ],
   "id": "b3fca4eb9624433d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:28:52.907484Z",
     "start_time": "2025-07-21T06:28:45.493977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "from collections import deque, Counter\n",
    "from fastai.vision.all import *\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ],
   "id": "7be6f00b1f85eb5d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the Device\n",
    "\n",
    "Ensure PyTorch is using the GPU if available, otherwise fallback to the CPU.\n"
   ],
   "id": "624666692439e5f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:29:14.122877Z",
     "start_time": "2025-07-21T06:29:14.117384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure PyTorch is using GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")"
   ],
   "id": "4b31c5097833e4a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 16:29:14,119 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading Pre-trained Models and Processors\n",
    "\n",
    "Load the pre-trained emotion and age detection models along with their respective processors from Hugging Face.\n",
    "We are using the `trpakov/vit-face-expression` model for emotion detection and the `nateraw/vit-age-classifier` model for age detection.\n",
    "We have enabled fast tokenization for the image processors to speed up the inference, which might impact the accuracy slightly.\n"
   ],
   "id": "cf30bdc88d40a1a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:29:46.741030Z",
     "start_time": "2025-07-21T06:29:20.955607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the pre-trained emotion model and image processor from Hugging Face\n",
    "emotion_model_name = \"trpakov/vit-face-expression\"\n",
    "logging.info(f\"Loading emotion model: {emotion_model_name}\")\n",
    "emotion_model = AutoModelForImageClassification.from_pretrained(emotion_model_name).to(device)\n",
    "emotion_processor = AutoImageProcessor.from_pretrained(emotion_model_name, use_fast=True)\n",
    "\n",
    "# Load the pre-trained age detection model and processor from Hugging Face\n",
    "age_model_name = \"nateraw/vit-age-classifier\"\n",
    "logging.info(f\"Loading age model: {age_model_name}\")\n",
    "age_model = AutoModelForImageClassification.from_pretrained(age_model_name).to(device)\n",
    "age_processor = AutoImageProcessor.from_pretrained(age_model_name, use_fast=True)"
   ],
   "id": "f3922c5d2544aed5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 16:29:20,958 - INFO - Loading emotion model: trpakov/vit-face-expression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/915 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0615d73ae8040848367344528d812d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-07-21 16:29:22,112 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1f61dd8cadd47d5a9376c125f3c8691"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa9a62041aca4c55ac9896ac36d60dfb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 16:29:30,596 - INFO - Loading age model: nateraw/vit-age-classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/850 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35a65a8336f2418fb12eaad9f18b0408"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-07-21 16:29:31,448 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81e11545153a4ee7861761ef6870c7c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/197 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53d2233d2f5a43adae46418cf2d6f845"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up a Rolling Window for Emotions\n",
    "\n",
    "Define a rolling window size and initialize a deque to store detected emotions.\n"
   ],
   "id": "115947bbe6cd7d41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:29:56.738887Z",
     "start_time": "2025-07-21T06:29:56.734591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the rolling window size\n",
    "ROLLING_WINDOW_SIZE = 15\n",
    "\n",
    "# Initialize deques to store detected emotions and ages\n",
    "emotion_window = deque(maxlen=ROLLING_WINDOW_SIZE)\n",
    "age_window = deque(maxlen=ROLLING_WINDOW_SIZE)"
   ],
   "id": "92d78c06b0b57e9b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading Haar Cascade for Face Detection\n",
    "\n",
    "Load OpenCV's pre-trained Haar Cascade for face detection.\n",
    "\n",
    "We might switch over to a model from Hugging Face for face detection in the future.\n",
    "\n",
    "But Haar Cascade is a simple and fast method for face detection, and is sufficient for this demonstration.\n"
   ],
   "id": "194136ef7aee2f52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:30:13.385406Z",
     "start_time": "2025-07-21T06:30:13.353237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load OpenCV's pre-trained Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ],
   "id": "fd1917d44701ccbe",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining Helper Functions\n",
    "\n",
    "Define functions for detecting blue images, getting the active camera, and processing faces. We have a range of helper functions to draw text with background, save and load the camera index, and get the active camera. We also created a function to help make the text drawn about the detected faces more readable. \n"
   ],
   "id": "df36dc370ee5ac28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:30:23.073891Z",
     "start_time": "2025-07-21T06:30:23.063617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_image_blue(frame, blue_threshold=50):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    lower_blue = np.array([100, 150, 0])\n",
    "    upper_blue = np.array([140, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "    blue_area = np.sum(mask > 0)\n",
    "    total_area = mask.shape[0] * mask.shape[1]\n",
    "    blue_percentage = (blue_area / total_area) * 100\n",
    "    return blue_percentage > blue_threshold\n",
    "\n",
    "\n",
    "CAMERA_INDEX_FILE = \"camera_index.txt\"\n",
    "\n",
    "\n",
    "def save_camera_index(index):\n",
    "    with open(CAMERA_INDEX_FILE, \"w\") as f:\n",
    "        f.write(str(index))\n",
    "\n",
    "\n",
    "def load_camera_index():\n",
    "    if os.path.exists(CAMERA_INDEX_FILE):\n",
    "        with open(CAMERA_INDEX_FILE, \"r\") as f:\n",
    "            return int(f.read())\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_active_camera(max_cameras=10, blue_threshold=50):\n",
    "    logging.info(f\"Checking up to {max_cameras} cameras for activity...\")\n",
    "\n",
    "    recent_camera_index = load_camera_index()\n",
    "    if recent_camera_index is not None:\n",
    "        logging.info(f\"Active camera found at recent index: {recent_camera_index}\")\n",
    "        return recent_camera_index\n",
    "\n",
    "    for camera_index in range(max_cameras):\n",
    "        logging.info(f\"Checking camera index: {camera_index}\")\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        if cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "            if ret:\n",
    "                if not is_image_blue(frame, blue_threshold):\n",
    "                    logging.info(f\"Active camera found at index: {camera_index}\")\n",
    "                    save_camera_index(camera_index)\n",
    "                    return camera_index\n",
    "    logging.warning(\"No active camera found that is not predominantly blue.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Define color constants\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "\n",
    "\n",
    "def draw_text_with_background(frame, text, position, font_scale=0.6, thickness=2, text_color=(255, 255, 255),\n",
    "                              bg_color=(0, 0, 0)):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    x, y = position\n",
    "\n",
    "    # Get text size\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "    text_w, text_h = text_size\n",
    "\n",
    "    # Draw background rectangle\n",
    "    cv2.rectangle(frame, (x, y - text_h - 10), (x + text_w + 10, y + 10), bg_color, cv2.FILLED)\n",
    "\n",
    "    # Draw text\n",
    "    cv2.putText(frame, text, (x + 5, y - 5), font, font_scale, text_color, thickness, cv2.LINE_AA)"
   ],
   "id": "8b7a3828b85264cb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing Faces\n",
    "\n",
    "Define a function to process each face in the frame for emotion and age detection. We use the emotion and age models to predict the emotion and age of the detected face. We also store the detected emotions and ages in a rolling window and only display the most common emotion and age if the scores are above the defined thresholds.\n",
    "\n",
    "The rolling window helps to stabilize the detected emotions and ages by considering the most common emotion and age in the window.\n"
   ],
   "id": "380993e71a050aea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:30:30.037953Z",
     "start_time": "2025-07-21T06:30:30.029561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMOTION_SCORE_THRESHOLD = 70\n",
    "AGE_SCORE_THRESHOLD = 50\n",
    "\n",
    "\n",
    "def process_face(face_img, emotion_model, emotion_processor, age_model, age_processor):\n",
    "    face_img_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "    face_img_rgb = PILImage.create(face_img_rgb)\n",
    "    emotion_inputs = emotion_processor(images=[face_img_rgb], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emotion_outputs = emotion_model(**emotion_inputs)\n",
    "        emotion_probs = torch.nn.functional.softmax(emotion_outputs.logits, dim=-1)\n",
    "        predicted_emotion_class = torch.argmax(emotion_probs, dim=-1)\n",
    "\n",
    "    emotion_labels = emotion_model.config.id2label\n",
    "    emotion = emotion_labels[predicted_emotion_class.item()]\n",
    "    emotion_score = emotion_probs[0, predicted_emotion_class].item()\n",
    "\n",
    "    age_inputs = age_processor(images=[face_img_rgb], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        age_outputs = age_model(**age_inputs)\n",
    "        age_probs = torch.nn.functional.softmax(age_outputs.logits, dim=-1)\n",
    "        predicted_age_class = torch.argmax(age_probs, dim=-1)\n",
    "\n",
    "    age_labels = age_model.config.id2label\n",
    "    age = age_labels[predicted_age_class.item()]\n",
    "    age_score = age_probs[0, predicted_age_class].item()\n",
    "\n",
    "    # Only append and process emotion and age if the scores are 80% or more\n",
    "\n",
    "    most_common_emotion = None\n",
    "    most_common_age = None\n",
    "\n",
    "    # convert emotion and age scores to percentages\n",
    "    emotion_score *= 100\n",
    "    age_score *= 100\n",
    "\n",
    "    if emotion_score >= EMOTION_SCORE_THRESHOLD:\n",
    "        emotion_window.append(emotion)\n",
    "        most_common_emotion = Counter(emotion_window).most_common(1)[0][0]\n",
    "\n",
    "    if age_score >= AGE_SCORE_THRESHOLD:\n",
    "        age_window.append(age)\n",
    "        most_common_age = Counter(age_window).most_common(1)[0][0]\n",
    "\n",
    "    return most_common_emotion, emotion_score, most_common_age, age_score"
   ],
   "id": "fa968dd2cd3f7236",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing Frames\n",
    "\n",
    "Define a function to process each frame from the webcam feed. Here is where we detect faces using Haar Cascade and process each face for emotion and age detection. We highlight the detected face with a rectangle and display the estimated emotion and age only if both emotion and age scores are above the defined thresholds. We also display the number of faces found in the frame.\n"
   ],
   "id": "5576adc268003fbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:30:37.706343Z",
     "start_time": "2025-07-21T06:30:37.699268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FONT_SCALE = 0.5\n",
    "\n",
    "\n",
    "def process_frame(frame, emotion_model, emotion_processor, age_model, age_processor):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    face_count = len(faces)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = frame[y:y + h, x:x + w]\n",
    "        most_common_emotion, emotion_score, most_common_age, age_score = process_face(face_img, emotion_model,\n",
    "                                                                                      emotion_processor, age_model,\n",
    "                                                                                      age_processor)\n",
    "\n",
    "        if most_common_emotion and most_common_age:\n",
    "            # Highlight the detected face with a rectangle and show the estimated emotion and age\n",
    "            # only if both emotion and age scores are above the thresholds\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), WHITE, 2)\n",
    "\n",
    "            draw_text_with_background(frame, f'Emotion: {most_common_emotion} ({emotion_score:.1f}%)',\n",
    "                                      (x, y - 10), text_color=WHITE, bg_color=BLUE)\n",
    "            draw_text_with_background(frame, f'Age: {most_common_age} ({age_score:.1f}%)',\n",
    "                                      (x, y + 20), text_color=WHITE, bg_color=RED)\n",
    "\n",
    "    draw_text_with_background(frame, f'Faces found: {face_count}', (10, 30), text_color=GREEN, bg_color=BLACK)\n",
    "    return frame\n"
   ],
   "id": "b662022f3a745b31",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Main Function\n",
    "\n",
    "Define the main function to capture and process frames from the active camera. We display the processed frame with real-time emotion and age detection. Press 'q' to quit the application.\n"
   ],
   "id": "fe20bce99b8266e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:30:49.707138Z",
     "start_time": "2025-07-21T06:30:49.700674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WIDTH = 640\n",
    "HEIGHT = 480\n",
    "\n",
    "def main():\n",
    "    active_camera = get_active_camera()\n",
    "    if active_camera is None:\n",
    "        logging.error(\"No active camera found that is not predominantly blue.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    cap = cv2.VideoCapture(active_camera)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)\n",
    "\n",
    "    cv2.namedWindow('Webcam', cv2.WINDOW_NORMAL)\n",
    "    cv2.setWindowProperty('Webcam', cv2.WND_PROP_TOPMOST, 1)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        processed_frame = process_frame(frame, emotion_model, emotion_processor, age_model, age_processor)\n",
    "        cv2.imshow('Webcam', processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ],
   "id": "f6497aa57cdf5ca1",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run the Application\n",
    "\n",
    "Execute the main function to start the webcam feed and perform real-time emotion and age detection. Press 'q' to quit the application.\n"
   ],
   "id": "75641752c1de2cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T06:33:13.111844Z",
     "start_time": "2025-07-21T06:30:54.797324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "42a92a125d9925d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 16:30:54,799 - INFO - Checking up to 10 cameras for activity...\n",
      "2025-07-21 16:30:54,800 - INFO - Active camera found at recent index: 0\n",
      "E:\\GitHubProjects\\FaceFinder\\.venv\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
